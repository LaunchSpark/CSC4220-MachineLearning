{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!--\n## Part 1: Hand Calculations\n\n### Problem 1.1: Closed-Form Solution\nFor \\(f(x) = x^2 - 4x + 7\\),\n\n\\(f'(x) = 2x - 4\\). Setting \\(f'(x)=0\\) gives \\(2x-4=0 \\Rightarrow x=2\\).\nSince \\(f''(x)=2>0\\), this is a minimum. The minimum value is \\(f(2)=4-8+7=3\\).\n\n### Problem 1.2: Gradient Descent by Hand\nUpdate rule: \\(x_{n+1} = x_n - \\alpha f'(x_n)\\), with \\(x_0=0\\), \\(\\alpha=0.1\\), \\(f'(x)=2x-4\\).\n\n| iteration (n) | \\(x_n\\) | \\(f'(x_n)\\) | \\(f(x_n)\\) |\n|---:|---:|---:|---:|\n| 0 | 0.0000 | -4.0000 | 7.0000 |\n| 1 | 0.4000 | -3.2000 | 5.5600 |\n| 2 | 0.7200 | -2.5600 | 4.6384 |\n| 3 | 0.9760 | -2.0480 | 4.0486 |\n| 4 | 1.1808 | -1.6384 | 3.6711 |\n\nAfter 5 updates, \\(x_5 = 1.34464\\). The true minimum is at \\(x=2\\), so the distance to the minimizer is \\(|1.34464-2|=0.65536\\).\n\nThe sign of \\(f'(x_n)\\) tells the direction to move (negative means move right, positive means move left). The magnitude \\(|f'(x_n)|\\) scales the step size so that when the slope is steep (far from the minimum), we take larger steps, and when the slope is small (near the minimum), we take smaller steps for stability.\n\n### Problem 1.3: Fence Optimization\nArea constraint: \\(LW = 43{,}560\\) so \\(W=\frac{43{,}560}{L}\\).\n\n**Perimeter (in terms of \\(L\\) only):**\n\\(P(L) = 2L + 2W = 2L + \frac{87{,}120}{L}\\).\n\n**Cost function:**\n\\(C(L) = 8P(L) = 16L + \frac{16\\cdot 43{,}560}{L}\\).\n\n**Domain:** \\(L>0\\) (positive length).\n\n**Closed-form minimum:**\n\\(C'(L)=16 - \frac{16\\cdot 43{,}560}{L^2}\\). Setting \\(C'(L)=0\\) gives \\(L^2=43{,}560\\), so\n\\(L=\\sqrt{43{,}560}\\approx 208.71\\) ft and \\(W=\frac{43{,}560}{L}=\\sqrt{43{,}560}\\approx 208.71\\) ft.\n\nMinimum total cost:\n\\(C(L) = 16L + \frac{16\\cdot 43{,}560}{L}\\Rightarrow C(\\sqrt{43{,}560})\\approx \\$6{,}678.73\\).\n\n**Observation:** The optimal fence is a square (length equals width).\n-->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c7c877",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.polynomial as poly\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dae7461",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent(coef, x_start, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    f: function to minimize\n",
        "    df: derivative of f\n",
        "    x_start: starting point\n",
        "    learning_rate: step size\n",
        "    num_iterations: number of iterations to run\n",
        "    \n",
        "    Returns: (x_history, f_history) - lists of x values and f(x) values\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    # delete this and the following line in your implementation. (do not delete the return)\n",
        "    f = poly.Polynomial(coef)\n",
        "    df = f.deriv()\n",
        "    x_history = [x_start]\n",
        "    f_history = [f(x_start)]\n",
        "\n",
        "    x = x_start\n",
        "    for i in range(num_iterations):\n",
        "        x -= learning_rate * float(df(x))\n",
        "        x_history.append(float(x))\n",
        "        f_history.append(float(f(x)))\n",
        "\n",
        "    return x_history, f_history\n",
        "\n",
        "\n",
        "# function2(x) = x^2 - 12x + 4\n",
        "coef_f2 = [4, -12, 1]\n",
        "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "results = {}\n",
        "for lr in learning_rates:\n",
        "    x_hist, f_hist = gradient_descent(coef_f2, x_start=0.0, learning_rate=lr, num_iterations=100)\n",
        "    results[lr] = (x_hist, f_hist)\n",
        "    plt.plot(range(len(f_hist)), f_hist, linewidth=2, label=f\"\u03b1 = {lr}\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.title(\"Gradient Descent Convergence for Different Learning Rates\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print final values again for convenience\n",
        "for lr in learning_rates:\n",
        "    x_hist, f_hist = results[lr]\n",
        "    print(f\"\u03b1 = {lr:0.2f} | final x = {x_hist[-1]:.10f} | final f(x) = {f_hist[-1]:.10f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b12f0eb",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}