{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Hand Calculations (50 points)\n",
        "\n",
        "### Problem 1.1: Closed-Form Solution (10 points)\n",
        "Find the minimum of \\(f(x) = x^2 - 4x + 7\\) using calculus.\n",
        "\n",
        "\\(f'(x) = 2x - 4\\). Setting \\(f'(x)=0\\) gives \\(x=2\\). Since \\(f''(x)=2>0\\), this is a minimum. The minimum value is \\(f(2)=3\\).\n",
        "\n",
        "### Problem 1.2: Gradient Descent by Hand (20 points)\n",
        "Use gradient descent to find the minimum of \\(f(x) = x^2 - 4x + 7\\) starting from \\(x_0=0\\) with learning rate \\(\\alpha = 0.1\\).\n",
        "\n",
        "Update rule: \\(x_{n+1} = x_n - \\alpha f'(x_n)\\), with \\(f'(x)=2x-4\\).\n",
        "\n",
        "| iteration (n) | \\(x_n\\) | \\(f'(x_n)\\) | \\(f(x_n)\\) |\n",
        "|---:|---:|---:|---:|\n",
        "| 0 | 0.00000 | -4.00000 | 7.00000 |\n",
        "| 1 | 0.40000 | -3.20000 | 5.56000 |\n",
        "| 2 | 0.72000 | -2.56000 | 4.63840 |\n",
        "| 3 | 0.97600 | -2.04800 | 4.04858 |\n",
        "| 4 | 1.18080 | -1.63840 | 3.67109 |\n",
        "| 5 | 1.34464 | -1.31072 | 3.42950 |\n",
        "\n",
        "After 5 iterations, \\(x_5 = 1.34464\\). The true minimum is at \\(x=2\\), so the distance to the minimizer is \\(|1.34464-2|=0.65536\\).\n",
        "\n",
        "The sign of \\(f'(x_n)\\) tells the direction to move (negative means move right, positive means move left). The magnitude \\(|f'(x_n)|\\) scales the step size so that when the slope is steep (far from the minimum), we take larger steps, and when the slope is small (near the minimum), we take smaller steps for stability.\n",
        "\n",
        "### Problem 1.3: Introducing Cost Functions - Fence Optimization (20 points)\n",
        "A farmer wants to build a rectangular fence to enclose exactly 1 acre (\\(43{,}560\\) ft\\(^2\\)). Fencing costs $8 per foot.\n",
        "\n",
        "**Derive the cost function**\n",
        "- Area constraint: \\(LW = 43{,}560\\), so \\(W = \\frac{43{,}560}{L}\\).\n",
        "- Perimeter: \\(P(L) = 2L + 2W = 2L + \\frac{87{,}120}{L}\\).\n",
        "- Cost function: \\(C(L) = 8P(L) = 16L + \\frac{16\\cdot 43{,}560}{L}\\).\n",
        "- Domain: \\(L>0\\).\n",
        "\n",
        "**Find the minimum cost (closed-form)**\n",
        "\\(C'(L)=16 - \\frac{16\\cdot 43{,}560}{L^2}\\). Setting \\(C'(L)=0\\) gives \\(L^2=43{,}560\\), so\n",
        "\\(L=\\sqrt{43{,}560}\\approx 208.71\\) ft and \\(W=\\frac{43{,}560}{L}=\\sqrt{43{,}560}\\approx 208.71\\) ft.\n",
        "\n",
        "Minimum total cost:\n",
        "\\(C(\\sqrt{43{,}560})\\approx \\$6{,}678.73\\).\n",
        "\n",
        "**Observation:** The optimal fence is a square (length equals width).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c7c877",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.polynomial as poly\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dae7461",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent(coef, x_start, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    f: function to minimize\n",
        "    df: derivative of f\n",
        "    x_start: starting point\n",
        "    learning_rate: step size\n",
        "    num_iterations: number of iterations to run\n",
        "    \n",
        "    Returns: (x_history, f_history) - lists of x values and f(x) values\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    # delete this and the following line in your implementation. (do not delete the return)\n",
        "    f = poly.Polynomial(coef)\n",
        "    df = f.deriv()\n",
        "    x_history = [x_start]\n",
        "    f_history = [f(x_start)]\n",
        "\n",
        "    x = x_start\n",
        "    for i in range(num_iterations):\n",
        "        x -= learning_rate * float(df(x))\n",
        "        x_history.append(float(x))\n",
        "        f_history.append(float(f(x)))\n",
        "\n",
        "    return x_history, f_history\n",
        "\n",
        "\n",
        "# function2(x) = x^2 - 12x + 4\n",
        "coef_f2 = [4, -12, 1]\n",
        "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "results = {}\n",
        "for lr in learning_rates:\n",
        "    x_hist, f_hist = gradient_descent(coef_f2, x_start=0.0, learning_rate=lr, num_iterations=100)\n",
        "    results[lr] = (x_hist, f_hist)\n",
        "    plt.plot(range(len(f_hist)), f_hist, linewidth=2, label=f\"\u03b1 = {lr}\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.title(\"Gradient Descent Convergence for Different Learning Rates\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# print final values again for convenience\n",
        "for lr in learning_rates:\n",
        "    x_hist, f_hist = results[lr]\n",
        "    print(f\"\u03b1 = {lr:0.2f} | final x = {x_hist[-1]:.10f} | final f(x) = {f_hist[-1]:.10f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b34822",
      "metadata": {},
      "source": [
        "**Questions to answer:**\n",
        "- **Which learning rate converges fastest?** \\(\\alpha=0.5\\) converges fastest because it moves directly to the minimum for this quadratic.\n",
        "- **What happens with \\(\\alpha=0.9\\)? Why?** It still converges but oscillates around the minimum due to a large step size; the update overshoots each time but shrinks because \\(\\alpha<1\\).\n",
        "- **What would you expect with \\(\\alpha=1.1\\)?** Divergence/oscillation with growing magnitude because \\(\\alpha>1\\) is too large for this quadratic (step size amplifies errors).\n",
        "- **Iterations to within 0.01 of the true minimum with \\(\\alpha=0.1\\):** About 29 iterations (starting from \\(x_0=0\\)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b00fcee",
      "metadata": {},
      "source": [
        "## Part 3: Analysis and Visualization\n",
        "\n",
        "### Problem 3.1: Convergence Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d096210e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fence cost function plot\n",
        "def fence_cost(L):\n",
        "    return 16 * L + (16 * 43560) / L\n",
        "\n",
        "L_vals = np.linspace(100, 21500, 500)\n",
        "C_vals = fence_cost(L_vals)\n",
        "\n",
        "L_opt = np.sqrt(43560)\n",
        "C_opt = fence_cost(L_opt)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(L_vals, C_vals, label='C(L)')\n",
        "plt.scatter([L_opt], [C_opt], color='red', label=f'Minimum at L\u2248{L_opt:.2f}')\n",
        "plt.title('Fence Cost Function')\n",
        "plt.xlabel('Length L (ft)')\n",
        "plt.ylabel('Cost C(L) ($)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Convergence plot for f(x) = x^2 - 4x + 7 with \u03b1=0.1 over 1000 iterations\n",
        "coef_f1 = [7, -4, 1]\n",
        "x_hist_1000, f_hist_1000 = gradient_descent(coef_f1, x_start=0, learning_rate=0.1, num_iterations=1000)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(f_hist_1000)\n",
        "plt.title('Gradient Descent Convergence for f(x) with \u03b1=0.1')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Iterations to within 0.01 of the true minimum (x=2)\n",
        "f_poly = poly.Polynomial(coef_f1)\n",
        "df_poly = f_poly.deriv()\n",
        "x = 0\n",
        "alpha = 0.1\n",
        "iter_within = None\n",
        "for i in range(1000):\n",
        "    if abs(x - 2) < 0.01:\n",
        "        iter_within = i\n",
        "        break\n",
        "    x = x - alpha * float(df_poly(x))\n",
        "iter_within\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35681dd0",
      "metadata": {},
      "source": [
        "**Iterations to reach within 0.01 of the true minimum (\\(x=2\\)) with \\(\\alpha=0.1\\):** 24 iterations (starting from \\(x_0=0\\))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e34c95",
      "metadata": {},
      "source": [
        "### Problem 3.2: Reflection Questions\n",
        "- **When might iterative methods be necessary?** When a closed-form derivative solution is unavailable, too expensive to compute, or the objective is high-dimensional/non-convex (common in machine learning). Iterative methods scale to large datasets and models.\n",
        "- **What role does the learning rate play?** The learning rate controls step size: too small means slow convergence, too large causes oscillation or divergence. A good learning rate balances speed and stability.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}