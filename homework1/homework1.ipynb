{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!--\n",
        "## Part 1: Hand Calculations\n\n### Problem 1.1: Closed-Form Solution\nFor \\(f(x) = x^2 - 4x + 7\\),\n\n\\(f'(x) = 2x - 4\\). Setting \\(f'(x)=0\\) gives \\(2x-4=0 \\Rightarrow x=2\\).\nSince \\(f''(x)=2>0\\), this is a minimum. The minimum value is \\(f(2)=4-8+7=3\\).\n\n### Problem 1.2: Gradient Descent by Hand\nUpdate rule: \\(x_{n+1} = x_n - \\alpha f'(x_n)\\), with \\(x_0=0\\), \\(\\alpha=0.1\\), \\(f'(x)=2x-4\\).\n\n| iteration (n) | \\(x_n\\) | \\(f'(x_n)\\) | \\(f(x_n)\\) |\n|---:|---:|---:|---:|\n| 0 | 0.0000 | -4.0000 | 7.0000 |\n| 1 | 0.4000 | -3.2000 | 5.5600 |\n| 2 | 0.7200 | -2.5600 | 4.6384 |\n| 3 | 0.9760 | -2.0480 | 4.0486 |\n| 4 | 1.1808 | -1.6384 | 3.6711 |\n\nAfter 5 updates, \\(x_5 = 1.34464\\). The true minimum is at \\(x=2\\), so the distance to the minimizer is \\(|1.34464-2|=0.65536\\).\n\nThe sign of \\(f'(x_n)\\) tells the direction to move (negative means move right, positive means move left). The magnitude \\(|f'(x_n)|\\) scales the step size so that when the slope is steep (far from the minimum), we take larger steps, and when the slope is small (near the minimum), we take smaller steps for stability.\n\n### Problem 1.3: Fence Optimization\nArea constraint: \\(LW = 43{,}560\\) so \\(W=\frac{43{,}560}{L}\\).\n\n**Perimeter (in terms of \\(L\\) only):**\n\\(P(L) = 2L + 2W = 2L + \frac{87{,}120}{L}\\).\n\n**Cost function:**\n\\(C(L) = 8P(L) = 16L + \frac{16\\cdot 43{,}560}{L}\\).\n\n**Domain:** \\(L>0\\) (positive length).\n\n**Closed-form minimum:**\n\\(C'(L)=16 - \frac{16\\cdot 43{,}560}{L^2}\\). Setting \\(C'(L)=0\\) gives \\(L^2=43{,}560\\), so\n\\(L=\\sqrt{43{,}560}\\approx 208.71\\) ft and \\(W=\frac{43{,}560}{L}=\\sqrt{43{,}560}\\approx 208.71\\) ft.\n\nMinimum total cost:\n\\(C(L) = 16L + \frac{16\\cdot 43{,}560}{L}\\Rightarrow C(\\sqrt{43{,}560})\\approx \\$6{,}678.73\\).\n\n**Observation:** The optimal fence is a square (length equals width).\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Python Implementation\n",
        "\n",
        "### Problem 2.1: Implement Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def function1(x):\n",
        "    return x**2 - 4*x + 7\n",
        "\n",
        "def derivative1(x):\n",
        "    return 2*x - 4\n",
        "\n",
        "def gradient_descent(coef, x_start, learning_rate, num_iterations):\n",
        "    func, dfunc = coef\n",
        "    x = x_start\n",
        "    x_history = []\n",
        "    f_history = []\n",
        "    for _ in range(num_iterations):\n",
        "        x_history.append(x)\n",
        "        f_history.append(func(x))\n",
        "        x = x - learning_rate * dfunc(x)\n",
        "    return x_history, f_history\n",
        "\n",
        "x_history_1, f_history_1 = gradient_descent((function1, derivative1), x_start=0, learning_rate=0.1, num_iterations=50)\n",
        "x_history_1[-1], f_history_1[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2.2: Explore Different Learning Rates\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def function2(x):\n",
        "    return (x - 1)**2 - 10*x + 3\n",
        "\n",
        "def derivative2(x):\n",
        "    # function2 simplifies to x^2 - 12x + 4\n",
        "    return 2*x - 12\n",
        "\n",
        "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    x_hist, f_hist = gradient_descent((function2, derivative2), x_start=0, learning_rate=lr, num_iterations=100)\n",
        "    results[lr] = {\n",
        "        'x_final': x_hist[-1],\n",
        "        'f_final': f_hist[-1],\n",
        "        'x_hist': x_hist,\n",
        "        'f_hist': f_hist,\n",
        "    }\n",
        "\n",
        "# Plot convergence for each learning rate\n",
        "plt.figure(figsize=(8, 5))\n",
        "for lr in learning_rates:\n",
        "    plt.plot(results[lr]['f_hist'], label=f'\u03b1={lr}')\n",
        "plt.title('Function 2 Convergence by Learning Rate')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('f(x)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Final values after 100 iterations (starting from \\(x_0=0\\)):**\n",
        "- \\(\\alpha=0.01\\): \\(x\\approx 5.2043\\), \\(f(x)\\approx -31.3668\\)\n",
        "- \\(\\alpha=0.1\\): \\(x\\approx 6.0000\\), \\(f(x)\\approx -32.0000\\)\n",
        "- \\(\\alpha=0.5\\): \\(x=6.0000\\), \\(f(x)=-32.0000\\)\n",
        "- \\(\\alpha=0.9\\): \\(x\\approx 6.0000\\), \\(f(x)\\approx -32.0000\\)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Questions to answer:**\n",
        "- **Which learning rate converges fastest?** \\(\\alpha=0.5\\) converges fastest because it moves directly to the minimum for this quadratic.\n",
        "- **What happens with \\(\\alpha=0.9\\)? Why?** It still converges but oscillates around the minimum due to a large step size; the update overshoots each time but shrinks because \\(\\alpha<1\\).\n",
        "- **What would you expect with \\(\\alpha=1.1\\)?** Divergence/oscillation with growing magnitude because \\(\\alpha>1\\) is too large for this quadratic (step size amplifies errors).\n",
        "- **Iterations to within 0.01 of the true minimum with \\(\\alpha=0.1\\):** About 29 iterations (starting from \\(x_0=0\\)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Analysis and Visualization\n",
        "\n",
        "### Problem 3.1: Convergence Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fence cost function plot\n",
        "def fence_cost(L):\n",
        "    return 16*L + (16*43560)/L\n",
        "\n",
        "L_vals = np.linspace(100, 21500, 500)\n",
        "C_vals = fence_cost(L_vals)\n",
        "\n",
        "L_opt = np.sqrt(43560)\n",
        "C_opt = fence_cost(L_opt)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(L_vals, C_vals, label='C(L)')\n",
        "plt.scatter([L_opt], [C_opt], color='red', label=f'Minimum at L\u2248{L_opt:.2f}')\n",
        "plt.title('Fence Cost Function')\n",
        "plt.xlabel('Length L (ft)')\n",
        "plt.ylabel('Cost C(L) ($)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Convergence plot for function1 with \u03b1=0.1 over 1000 iterations\n",
        "x_hist_1000, f_hist_1000 = gradient_descent((function1, derivative1), x_start=0, learning_rate=0.1, num_iterations=1000)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(f_hist_1000)\n",
        "plt.title('Gradient Descent Convergence for f(x) with \u03b1=0.1')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Iterations to within 0.01 of the true minimum (x=2)\n",
        "x = 0\n",
        "alpha = 0.1\n",
        "iter_within = None\n",
        "for i in range(1000):\n",
        "    if abs(x - 2) < 0.01:\n",
        "        iter_within = i\n",
        "        break\n",
        "    x = x - alpha * derivative1(x)\n",
        "iter_within"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Iterations to reach within 0.01 of the true minimum (\\(x=2\\)) with \\(\\alpha=0.1\\):** 24 iterations (starting from \\(x_0=0\\)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3.2: Reflection Questions\n",
        "- **When might iterative methods be necessary?** When a closed-form derivative solution is unavailable, too expensive to compute, or the objective is high-dimensional/non-convex (common in machine learning). Iterative methods scale to large datasets and models.\n",
        "- **What role does the learning rate play?** The learning rate controls step size: too small means slow convergence, too large causes oscillation or divergence. A good learning rate balances speed and stability.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}