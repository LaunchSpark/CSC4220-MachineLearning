{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "Linear Regression with Gradient Descent (from scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Only for data loading and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent\n",
    "    Model: y_hat = m * x + b\n",
    "\n",
    "    This implementation follows the mathematical framework from Part 1:\n",
    "    - Compute gradient contribution from each data point\n",
    "    - Sum all contributions to get total gradient\n",
    "    - Update parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        \"\"\"\n",
    "        learning_rate: step size for gradient descent (\u03b1)\n",
    "        num_iterations: number of iterations to run\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.m = 0  # slope\n",
    "        self.b = 0  # intercept\n",
    "        self.loss_history = []  # track loss over iterations\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Make a prediction for a single input value\n",
    "\n",
    "        Input:\n",
    "            x: a single input value (scalar)\n",
    "        Returns:\n",
    "            y_hat: predicted value (scalar)\n",
    "        \"\"\"\n",
    "        return self.m * x + self.b\n",
    "\n",
    "    def optimize(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "\n",
    "        Inputs:\n",
    "            X: list or array of input values\n",
    "            y: list or array of true output values\n",
    "        Returns:\n",
    "            self (for method chaining)\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        self.loss_history = []\n",
    "\n",
    "        for iteration in range(self.num_iterations):\n",
    "            grad_m = 0.0\n",
    "            grad_b = 0.0\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Loop over each data point individually (no vectorized gradients)\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                y_hat = self.predict(x_i)\n",
    "                error = y_hat - y_i\n",
    "\n",
    "                grad_m += (2.0 / n) * error * x_i\n",
    "                grad_b += (2.0 / n) * error\n",
    "                total_loss += error ** 2\n",
    "\n",
    "            self.m -= self.learning_rate * grad_m\n",
    "            self.b -= self.learning_rate * grad_b\n",
    "\n",
    "            self.loss_history.append(total_loss / n)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Problem 1.1\n",
    "X_small = [2.5, 3.0, 3.5, 4.0, 4.5]\n",
    "y_small = [30, 25, 22, 18, 15]\n",
    "\n",
    "# Train model\n",
    "model = LinearRegressionGD(learning_rate=0.01, num_iterations=100)\n",
    "model.optimize(X_small, y_small)\n",
    "\n",
    "print(f\"\\nFinal parameters:\")\n",
    "print(f\"m (slope): {model.m:.4f}\")\n",
    "print(f\"b (intercept): {model.b:.4f}\")\n",
    "print(f\"Final Loss: {model.loss_history[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: 1 iteration from m=-5, b=40\n",
    "verify_model = LinearRegressionGD(learning_rate=0.01, num_iterations=1)\n",
    "verify_model.m = -5\n",
    "verify_model.b = 40\n",
    "verify_model.optimize(X_small, y_small)\n",
    "\n",
    "print(\"After 1 iteration with lr=0.01 from m=-5, b=40:\")\n",
    "print(f\"m: {verify_model.m:.4f}\")\n",
    "print(f\"b: {verify_model.b:.4f}\")\n",
    "print(\"Compare these values to your hand calculation from Problem 1.4 Part B Q3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Auto MPG dataset\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "urlretrieve(url, 'auto-mpg.data')\n",
    "\n",
    "# Load data\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "                'acceleration', 'model_year', 'origin', 'car_name']\n",
    "data = pd.read_csv('auto-mpg.data', names=column_names,\n",
    "                   delim_whitespace=True, na_values='?')\n",
    "\n",
    "# Extract weight (in lbs) and mpg, remove missing values\n",
    "data = data.dropna()\n",
    "X_full = (data['weight'].values / 1000).tolist()  # Convert to 1000s of lbs, then to list\n",
    "y_full = data['mpg'].values.tolist()  # Convert to list\n",
    "\n",
    "print(f\"Dataset size: {len(X_full)} vehicles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on full dataset\n",
    "# Start with the same learning rate as before and adjust if needed\n",
    "model_full = LinearRegressionGD(learning_rate=0.01, num_iterations=1000)\n",
    "model_full.optimize(X_full, y_full)\n",
    "\n",
    "print(f\"\\nFull dataset - Final parameters:\")\n",
    "print(f\"m (slope): {model_full.m:.4f}\")\n",
    "print(f\"b (intercept): {model_full.b:.4f}\")\n",
    "print(f\"Final Loss: {model_full.loss_history[-1]:.4f}\")\n",
    "\n",
    "x_test = 3.0\n",
    "print(f\"Predicted MPG for 3000 lbs (x=3.0): {model_full.predict(x_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2 Questions (fill in after running)\n",
    "- Slope interpretation: For every 1000 lb increase in vehicle weight, MPG changes by approximately `m` MPG.\n",
    "- Relationship sense-check: Heavier cars usually get lower MPG, so a negative slope is expected.\n",
    "- Learning rate note: Keep `0.01` if loss decreases smoothly; reduce it if the loss oscillates/diverges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGDMomentum:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent with Momentum\n",
    "\n",
    "    Momentum update rule (from Going Beyond 1.4):\n",
    "        v_m = \u03b2 * v_m + gradient_m\n",
    "        m = m - \u03b1 * v_m\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, momentum=0.9):\n",
    "        \"\"\"\n",
    "        learning_rate: step size \u03b1\n",
    "        num_iterations: number of iterations\n",
    "        momentum: momentum coefficient \u03b2 (typically 0.9)\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.momentum = momentum\n",
    "        self.m = 0\n",
    "        self.b = 0\n",
    "        self.v_m = 0  # velocity for m\n",
    "        self.v_b = 0  # velocity for b\n",
    "        self.loss_history = []\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Same as before\"\"\"\n",
    "        return self.m * x + self.b\n",
    "\n",
    "    def optimize(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using gradient descent with momentum\n",
    "\n",
    "        Inputs:\n",
    "            X: list or array of input values\n",
    "            y: list or array of true output values\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        self.loss_history = []\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            grad_m = 0.0\n",
    "            grad_b = 0.0\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for x_i, y_i in zip(X, y):\n",
    "                y_hat = self.predict(x_i)\n",
    "                error = y_hat - y_i\n",
    "\n",
    "                grad_m += (2.0 / n) * error * x_i\n",
    "                grad_b += (2.0 / n) * error\n",
    "                total_loss += error ** 2\n",
    "\n",
    "            self.v_m = self.momentum * self.v_m + grad_m\n",
    "            self.v_b = self.momentum * self.v_b + grad_b\n",
    "\n",
    "            self.m -= self.learning_rate * self.v_m\n",
    "            self.b -= self.learning_rate * self.v_b\n",
    "\n",
    "            self.loss_history.append(total_loss / n)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard gradient descent\n",
    "model_standard = LinearRegressionGD(learning_rate=0.01, num_iterations=1000)\n",
    "model_standard.optimize(X_full, y_full)\n",
    "\n",
    "# Gradient descent with momentum\n",
    "model_momentum = LinearRegressionGDMomentum(learning_rate=0.01, num_iterations=1000, momentum=0.9)\n",
    "model_momentum.optimize(X_full, y_full)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model_standard.loss_history, label='Standard GD', alpha=0.7)\n",
    "plt.plot(model_momentum.loss_history, label='GD with Momentum', alpha=0.7)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comparison: Standard vs Momentum')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Standard GD final loss: {model_standard.loss_history[-1]:.4f}\")\n",
    "print(f\"Momentum GD final loss: {model_momentum.loss_history[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3.1: Learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model_full.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error (Loss)')\n",
    "plt.title('Learning Curve: Loss vs Iteration')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3.2: Model fit visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot 1: Small dataset\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_small, y_small, color='blue', s=100, alpha=0.6, label='Data')\n",
    "# Plot the fitted line\n",
    "x_line = np.linspace(min(X_small), max(X_small), 100)\n",
    "y_line = [model.predict(x) for x in x_line]\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2, label=f'Fit: y = {model.m:.2f}x + {model.b:.2f}')\n",
    "plt.xlabel('Weight (1000s lbs)')\n",
    "plt.ylabel('MPG')\n",
    "plt.title('Small Dataset (5 points)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Full dataset\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_full, y_full, color='blue', s=20, alpha=0.4, label='Data')\n",
    "x_line = np.linspace(min(X_full), max(X_full), 100)\n",
    "y_line = [model_full.predict(x) for x in x_line]\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2,\n",
    "         label=f'Fit: y = {model_full.m:.2f}x + {model_full.b:.2f}')\n",
    "plt.xlabel('Weight (1000s lbs)')\n",
    "plt.ylabel('MPG')\n",
    "plt.title(f'Full Dataset ({len(X_full)} vehicles)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}